{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMeAEARlnReXOtiPlxZUJDo",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/UdaraChamidu/Large-Language-Models/blob/main/LLM_from_Scratch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Building a Large Language Model from Scratch**\n",
        "\n",
        "**Next Word Prediction**"
      ],
      "metadata": {
        "id": "siFf6vEctGng"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The goal of our language model will be to predict the next word."
      ],
      "metadata": {
        "id": "lr2nQufBtRM8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Main Steps**\n",
        "\n",
        "*   Tokenization\n",
        "*   Self-Attention\n",
        "*   Transformer Block\n",
        "*   Full Language Model\n",
        "*   Embedding Layer\n",
        "*   Positional Encoding\n"
      ],
      "metadata": {
        "id": "43df1OcBtXj2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Import packeges"
      ],
      "metadata": {
        "id": "NmT6_oSGuCp9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn       # nn=neural network\n",
        "import torch.optim as optim\n",
        "import math"
      ],
      "metadata": {
        "id": "Ik6ydGbhtQik"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 1: Tokenization\n"
      ],
      "metadata": {
        "id": "aoC5jtEZuLFQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "What:\n",
        "Converts text into tokens (usually subwords or characters). These tokens are then mapped to unique IDs using a vocabulary.\n",
        "\n",
        "Why:\n",
        "Models canâ€™t understand text directlyâ€”they understand numbers. Tokenization is the bridge.\n",
        "\n",
        "Example:\n",
        "\"Hello world\" â†’ [\"Hello\", \"world\"] â†’ [15496, 995]"
      ],
      "metadata": {
        "id": "lz2DpUJDw9af"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize(text, vocab):\n",
        "    return [vocab.get(word, vocab[\"\"]) for word in text.split()]\n",
        "\n",
        "# text.split()  -  split the sentences\n",
        "# vocab - a dictionary that gives numbers to words.\n",
        "# <UNK> - for unknown"
      ],
      "metadata": {
        "id": "JhAKDLMyuhYF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 2: Embedding Layer"
      ],
      "metadata": {
        "id": "GfV9uS8duNEL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "What:\n",
        "Converts token IDs into dense vectors (learned representations). Each token gets a vector of fixed size (e.g., 768-dim).\n",
        "\n",
        "Why:\n",
        "One-hot encodings are too sparse and donâ€™t capture relationships. Embeddings provide context-aware, continuous values."
      ],
      "metadata": {
        "id": "NlK9t7DMx28T"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "âœ… Why It Matters\n",
        "Itâ€™s your modelâ€™s first layer.\n",
        "\n",
        "Converts human-readable text into something the model can learn from.\n",
        "\n",
        "The quality of these embeddings directly impacts model performance."
      ],
      "metadata": {
        "id": "SEoaU-fE5aFP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Embedding(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim):\n",
        "        super(Embedding, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.embedding(x)\n",
        "\n",
        "# nn.Embedding - creates a table where each word id map to a vector.\n",
        "# embedding_dim - define the length of each vector\n",
        "# vocab_size - Number of unique tokens in your vocabulary (e.g., 50,000)\n",
        "# embedding_dim -  Size of the embedding vector for each token (e.g., 256 or 768)\n",
        "# x = torch.tensor([1, 42, 6])  # token IDs"
      ],
      "metadata": {
        "id": "GcVKY8xHuj_5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 3: Positional Encoding\n"
      ],
      "metadata": {
        "id": "D0pIU5b3uPlj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "What:\n",
        "Adds information about the position of each token in the sequence since the Transformer has no concept of order by default.\n",
        "\n",
        "Why:\n",
        "\"Dog bites man\" â‰  \"Man bites dog\" â€” order matters!\n",
        "\n",
        "How:\n",
        "Either fixed (sinusoidal) or learned embeddings are added to token embeddings."
      ],
      "metadata": {
        "id": "KntpWp8a4Dex"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The PositionalEncoding class adds position information to your input embeddings because Transformers donâ€™t have any built-in sense of word order."
      ],
      "metadata": {
        "id": "fvXwXEgh79WE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# transformers process all the words at once\n",
        "# which word should take after which word\n",
        "# position of the word ...\n",
        "\n",
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, embedding_dim, max_seq_len=5000):\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "        self.embedding_dim = embedding_dim\n",
        "        pe = torch.zeros(max_seq_len, embedding_dim) # store the positional encoding vectors\n",
        "        position = torch.arange(0, max_seq_len, dtype=torch.float).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, embedding_dim, 2).float() * (-math.log(10000.0) / embedding_dim))\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
        "        self.register_buffer('pe', pe)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return x + self.pe[:x.size(0), :]"
      ],
      "metadata": {
        "id": "uYjdTvR1umZq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 4: Self-Attention\n"
      ],
      "metadata": {
        "id": "n4v3pWIRuRRg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "**Give some extra focus on most important words.**\n",
        "-----------------------------------\n",
        "ðŸ§  What Is Self-Attention?\n",
        "\n",
        "It answers the question:\n",
        "\n",
        "â€œFor each word in the input, how much attention should I pay to every other word (including myself)?â€\n",
        "\n",
        "Why:\n",
        "It captures dependencies and relationships between words, no matter their distance.\n",
        "\n",
        "\n",
        "---------------------------------\n",
        "\n",
        "Query (Q): What I'm looking for   \n",
        "Key (K): What I contain          \n",
        "Value (V): What I offer if selected\n",
        "\n",
        "--------------------------------\n",
        "ðŸ§ª Example (Mini Visualization)\n",
        "Letâ€™s say we input the sentence:\n",
        "\n",
        "\"The cat sat on the mat\"\n",
        "\n",
        "When processing the word \"mat\", self-attention might focus more on \"sat\" and \"on\", because those are contextually related."
      ],
      "metadata": {
        "id": "KgkmZswe4GZ8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# heart of the Transformer â€” the Self-Attention mechanism\n",
        "\n",
        "class SelfAttention(nn.Module):\n",
        "    def __init__(self, embedding_dim):\n",
        "        super(SelfAttention, self).__init__()\n",
        "        self.query = nn.Linear(embedding_dim, embedding_dim)\n",
        "        self.key = nn.Linear(embedding_dim, embedding_dim)\n",
        "        self.value = nn.Linear(embedding_dim, embedding_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        queries = self.query(x)\n",
        "        keys = self.key(x)\n",
        "        values = self.value(x)\n",
        "        scores = torch.bmm(queries, keys.transpose(1, 2)) / torch.sqrt(torch.tensor(x.size(-1), dtype=torch.float32))\n",
        "        attention_weights = torch.softmax(scores, dim=-1)\n",
        "        attended_values = torch.bmm(attention_weights, values)\n",
        "        return attended_values\n",
        "\n",
        "# Softmax\t- Normalize attention scores"
      ],
      "metadata": {
        "id": "DbsrpDhxuo3B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 5: Transformer Block\n"
      ],
      "metadata": {
        "id": "3524U30-uRlD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Main part of any LLM (**brain**) **bold text**\n",
        "\n",
        "-----------------------------------\n",
        "ðŸ§± What is a Transformer Block?\n",
        "Itâ€™s a powerful block made of:\n",
        "\n",
        "Self-Attention\n",
        "\n",
        "Add & Norm (LayerNorm)\n",
        "\n",
        "Feedforward Network\n",
        "\n",
        "Another Add & Norm\n",
        "\n",
        "--------------------------------------\n",
        "\n",
        "ðŸ“Œ Why Residuals + LayerNorm?\n",
        "\n",
        "Concept\tPurpose\n",
        "Residual\tHelps with gradient flow and stability\n",
        "LayerNorm\tPrevents exploding/vanishing gradients, improves learning\n",
        "Feedforward\tAdds non-linearity and model capacity\n",
        "\n",
        "----------------------------------\n",
        "We stack many of these blocks to build deep models like GPT or BERT.\n",
        "Why:\n",
        "Itâ€™s the core building block that enables deep learning of language patterns.\n",
        "\n",
        "Stacked Blocks = Deep Model"
      ],
      "metadata": {
        "id": "Ka8LG3Et4LZK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerBlock(nn.Module):\n",
        "    def __init__(self, embedding_dim, hidden_dim):\n",
        "        super(TransformerBlock, self).__init__()\n",
        "        self.attention = SelfAttention(embedding_dim)\n",
        "        self.feed_forward = nn.Sequential(\n",
        "            nn.Linear(embedding_dim, hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_dim, embedding_dim)\n",
        "        )\n",
        "        self.norm1 = nn.LayerNorm(embedding_dim)\n",
        "        self.norm2 = nn.LayerNorm(embedding_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        attended = self.attention(x)\n",
        "        x = self.norm1(x + attended)\n",
        "        forwarded = self.feed_forward(x)\n",
        "        x = self.norm2(x + forwarded)\n",
        "        return x"
      ],
      "metadata": {
        "id": "n1SDzwZ6urCx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 6: Full Language Model\n"
      ],
      "metadata": {
        "id": "OZv4-PMTuXiO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "What:\n",
        "A stack of multiple Transformer blocks + output layer (usually softmax over vocabulary).\n",
        "\n",
        "Final output:\n",
        "Predicts the next token given a sequence.\n",
        "\n",
        "Example:\n",
        "Input: \"The cat sat on the\" â†’ Output: \"mat\" (most probable next token)"
      ],
      "metadata": {
        "id": "gSpCC8OQ4Phq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SimpleLLM(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim, hidden_dim, num_layers):\n",
        "        super(SimpleLLM, self).__init__()\n",
        "        self.embedding = Embedding(vocab_size, embedding_dim) # embedding layer\n",
        "        self.positional_encoding = PositionalEncoding(embedding_dim) # positional encoding layer\n",
        "        self.transformer_blocks = nn.Sequential(*[TransformerBlock(embedding_dim, hidden_dim) for _ in range(num_layers)])\n",
        "        self.output = nn.Linear(embedding_dim, vocab_size) # output layer\n",
        "\n",
        "    def forward(self, x):  # forware function\n",
        "        x = self.embedding(x)\n",
        "        x = x.transpose(0, 1) # Transpose for positional encoding\n",
        "        x = self.positional_encoding(x)\n",
        "        x = x.transpose(0, 1) # Transpose back\n",
        "        x = self.transformer_blocks(x)\n",
        "        x = self.output(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "6i6AwK0Nuvr5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 7: Training the Model\n"
      ],
      "metadata": {
        "id": "VONYuif_uYSF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Use the LLM for Eye Diseases"
      ],
      "metadata": {
        "id": "DdQgL7w1oYih"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Extract Text from PDF"
      ],
      "metadata": {
        "id": "lQJgHvNJofIg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install pymupdf\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LeVAADgYvL0C",
        "outputId": "48ac98b6-35b1-4d05-866d-474c0fb357fd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pymupdf in /usr/local/lib/python3.11/dist-packages (1.25.5)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import fitz  # PyMuPDF\n",
        "pdf_path = \"Eye Disease Classification Using DL.pdf\"\n",
        "def extract_text(pdf_path):\n",
        "    doc = fitz.open(pdf_path)\n",
        "    text = \"\"\n",
        "    for page in doc:\n",
        "        text += page.get_text()\n",
        "    return text\n",
        "\n",
        "text = extract_text(pdf_path)\n"
      ],
      "metadata": {
        "id": "5zuhLMfsocQC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Pre-tokenize Text"
      ],
      "metadata": {
        "id": "p0D-J2wvoqwU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "tokens = tokenizer.encode(text, return_tensors=\"pt\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zsa1_HF6ordB",
        "outputId": "af84dcb9-84a2-4f1c-967e-a7cbc0b297ec"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (4539 > 512). Running this sequence through the model will result in indexing errors\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- PDF Extraction ---\n",
        "import fitz  # PyMuPDF\n",
        "pdf_path = \"Eye Disease Classification Using DL.pdf\"\n",
        "\n",
        "def extract_text(pdf_path):\n",
        "    doc = fitz.open(pdf_path)\n",
        "    text = \"\"\n",
        "    for page in doc:\n",
        "        text += page.get_text()\n",
        "    return text\n",
        "\n",
        "text = extract_text(pdf_path)\n",
        "\n",
        "# --- Tokenizer ---\n",
        "from transformers import AutoTokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "\n",
        "# --- Simple sentence splitter (no nltk) ---\n",
        "sentences = [s.strip() for s in text.split('.') if s.strip()]\n",
        "\n",
        "# --- Tokenize sentences ---\n",
        "tokenized_data = [tokenizer.encode(sentence, add_special_tokens=False) for sentence in sentences]\n",
        "\n",
        "# --- LLM Setup ---\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "class SimpleLLM(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim, hidden_dim, num_layers):\n",
        "        super(SimpleLLM, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, num_layers, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_dim, vocab_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        embeds = self.embedding(x)\n",
        "        lstm_out, _ = self.lstm(embeds)\n",
        "        output = self.fc(lstm_out)\n",
        "        return output\n",
        "\n",
        "vocab_size = tokenizer.vocab_size\n",
        "embedding_dim = 16\n",
        "hidden_dim = 32\n",
        "num_layers = 2\n",
        "\n",
        "model = SimpleLLM(vocab_size, embedding_dim, hidden_dim, num_layers)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# --- Training ---\n",
        "for epoch in range(100):  # Reduce for testing\n",
        "    for sentence in tokenized_data:\n",
        "        if len(sentence) < 2:\n",
        "            continue\n",
        "        for i in range(1, len(sentence)):\n",
        "            input_seq = torch.tensor(sentence[:i]).unsqueeze(0)\n",
        "            target = torch.tensor(sentence[i]).unsqueeze(0)\n",
        "            optimizer.zero_grad()\n",
        "            output = model(input_seq)\n",
        "            loss = criterion(output[:, -1, :], target)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "    print(f\"Epoch {epoch}, Loss: {loss.item():.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "seE4bqWvxzmt",
        "outputId": "a42b97bf-8f4f-4787-e938-67d43fe8ffc5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0, Loss: 8.6590\n",
            "Epoch 1, Loss: 8.1820\n",
            "Epoch 2, Loss: 8.1228\n",
            "Epoch 3, Loss: 7.5126\n",
            "Epoch 4, Loss: 7.5573\n",
            "Epoch 5, Loss: 6.9281\n",
            "Epoch 6, Loss: 6.6246\n",
            "Epoch 7, Loss: 6.1776\n",
            "Epoch 8, Loss: 6.0890\n",
            "Epoch 9, Loss: 5.3231\n",
            "Epoch 10, Loss: 4.6248\n",
            "Epoch 11, Loss: 4.9848\n",
            "Epoch 12, Loss: 4.4115\n",
            "Epoch 13, Loss: 5.3112\n",
            "Epoch 14, Loss: 3.5056\n",
            "Epoch 15, Loss: 3.2357\n",
            "Epoch 16, Loss: 2.8213\n",
            "Epoch 17, Loss: 2.8089\n",
            "Epoch 18, Loss: 2.0741\n",
            "Epoch 19, Loss: 1.8925\n",
            "Epoch 20, Loss: 1.7744\n",
            "Epoch 21, Loss: 1.5223\n",
            "Epoch 22, Loss: 1.3088\n",
            "Epoch 23, Loss: 1.0387\n",
            "Epoch 24, Loss: 0.9783\n",
            "Epoch 25, Loss: 1.1201\n",
            "Epoch 26, Loss: 1.3037\n",
            "Epoch 27, Loss: 1.1164\n",
            "Epoch 28, Loss: 0.9398\n",
            "Epoch 29, Loss: 1.4343\n",
            "Epoch 30, Loss: 1.0297\n",
            "Epoch 31, Loss: 1.2250\n",
            "Epoch 32, Loss: 1.0371\n",
            "Epoch 33, Loss: 1.1217\n",
            "Epoch 34, Loss: 0.9630\n",
            "Epoch 35, Loss: 0.6536\n",
            "Epoch 36, Loss: 0.8835\n",
            "Epoch 37, Loss: 1.0958\n",
            "Epoch 38, Loss: 0.6149\n",
            "Epoch 39, Loss: 0.5326\n",
            "Epoch 40, Loss: 0.5916\n",
            "Epoch 41, Loss: 0.5154\n",
            "Epoch 42, Loss: 0.6545\n",
            "Epoch 43, Loss: 0.5694\n",
            "Epoch 44, Loss: 0.4389\n",
            "Epoch 45, Loss: 0.7158\n",
            "Epoch 46, Loss: 0.3493\n",
            "Epoch 47, Loss: 0.3563\n",
            "Epoch 48, Loss: 0.7354\n",
            "Epoch 49, Loss: 0.2616\n",
            "Epoch 50, Loss: 0.3855\n",
            "Epoch 51, Loss: 0.3122\n",
            "Epoch 52, Loss: 0.2034\n",
            "Epoch 53, Loss: 0.2210\n",
            "Epoch 54, Loss: 0.1840\n",
            "Epoch 55, Loss: 0.1269\n",
            "Epoch 56, Loss: 0.3262\n",
            "Epoch 57, Loss: 0.6610\n",
            "Epoch 58, Loss: 0.1446\n",
            "Epoch 59, Loss: 0.1846\n",
            "Epoch 60, Loss: 0.0970\n",
            "Epoch 61, Loss: 0.2485\n",
            "Epoch 62, Loss: 0.1407\n",
            "Epoch 63, Loss: 0.1661\n",
            "Epoch 64, Loss: 0.1050\n",
            "Epoch 65, Loss: 0.0880\n",
            "Epoch 66, Loss: 0.1203\n",
            "Epoch 67, Loss: 0.1142\n",
            "Epoch 68, Loss: 0.1892\n",
            "Epoch 69, Loss: 0.0622\n",
            "Epoch 70, Loss: 0.0935\n",
            "Epoch 71, Loss: 0.0472\n",
            "Epoch 72, Loss: 0.1427\n",
            "Epoch 73, Loss: 0.0894\n",
            "Epoch 74, Loss: 0.0440\n",
            "Epoch 75, Loss: 0.0677\n",
            "Epoch 76, Loss: 0.0880\n",
            "Epoch 77, Loss: 0.2347\n",
            "Epoch 78, Loss: 0.1555\n",
            "Epoch 79, Loss: 0.0199\n",
            "Epoch 80, Loss: 0.0375\n",
            "Epoch 81, Loss: 0.0555\n",
            "Epoch 82, Loss: 0.0189\n",
            "Epoch 83, Loss: 0.0705\n",
            "Epoch 84, Loss: 0.0413\n",
            "Epoch 85, Loss: 0.0616\n",
            "Epoch 86, Loss: 0.0286\n",
            "Epoch 87, Loss: 0.0299\n",
            "Epoch 88, Loss: 0.0541\n",
            "Epoch 89, Loss: 0.0208\n",
            "Epoch 90, Loss: 0.0602\n",
            "Epoch 91, Loss: 0.0536\n",
            "Epoch 92, Loss: 0.0139\n",
            "Epoch 93, Loss: 0.0964\n",
            "Epoch 94, Loss: 0.0114\n",
            "Epoch 95, Loss: 0.0232\n",
            "Epoch 96, Loss: 0.0217\n",
            "Epoch 97, Loss: 0.0842\n",
            "Epoch 98, Loss: 0.0684\n",
            "Epoch 99, Loss: 0.0290\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 8: Using the Model\n"
      ],
      "metadata": {
        "id": "8RGDbORfublX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "input_text = \"Convolution neural\"\n",
        "input_tokens = tokenizer.encode(input_text, add_special_tokens=False, return_tensors=\"pt\")\n",
        "output = model(input_tokens)\n",
        "predicted_token_id = torch.argmax(output[:, -1, :]).item()\n",
        "\n",
        "# Convert predicted token ID back to the word\n",
        "predicted_token = tokenizer.decode([predicted_token_id])\n",
        "\n",
        "print(f\"Input: {input_text}, Predicted next word: {predicted_token}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dASVSf6trlzc",
        "outputId": "8c777acd-a2c0-4f04-b74a-ca67c2487bfa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input: Convolution neural, Predicted next word: network\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "input_text = \"Eye disease classification using\"\n",
        "input_tokens = tokenizer.encode(input_text, add_special_tokens=False, return_tensors=\"pt\")\n",
        "output = model(input_tokens)\n",
        "predicted_token_id = torch.argmax(output[:, -1, :]).item()\n",
        "\n",
        "# Convert predicted token ID back to the word\n",
        "predicted_token = tokenizer.decode([predicted_token_id])\n",
        "\n",
        "print(f\"Input: {input_text}, Predicted next word: {predicted_token}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gAfZfikg_tbe",
        "outputId": "db3eca97-c3be-4f9f-b4d4-4a21e909c492"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input: Eye disease classification using, Predicted next word: deep\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "input_text = \"recall is an evaluation\"\n",
        "input_tokens = tokenizer.encode(input_text, add_special_tokens=False, return_tensors=\"pt\")\n",
        "output = model(input_tokens)\n",
        "predicted_token_id = torch.argmax(output[:, -1, :]).item()\n",
        "\n",
        "# Convert predicted token ID back to the word\n",
        "predicted_token = tokenizer.decode([predicted_token_id])\n",
        "\n",
        "print(f\"Input: {input_text}, Predicted next word: {predicted_token}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sZ0V-zPbh8ej",
        "outputId": "4a9e415c-49bf-4ab3-c93d-b1210eb7abbc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input: recall is an evaluation, Predicted next word: by\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "input_text = \"sri lanka is a beautiful\"\n",
        "input_tokens = tokenizer.encode(input_text, add_special_tokens=False, return_tensors=\"pt\")\n",
        "output = model(input_tokens)\n",
        "predicted_token_id = torch.argmax(output[:, -1, :]).item()\n",
        "\n",
        "# Convert predicted token ID back to the word\n",
        "predicted_token = tokenizer.decode([predicted_token_id])\n",
        "\n",
        "print(f\"Input: {input_text}, Predicted next word: {predicted_token}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w5yJRDrjiE4i",
        "outputId": "bfaa730c-e965-4289-96b5-afa4181c9ec0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input: sri lanka is a beautiful, Predicted next word: health\n"
          ]
        }
      ]
    }
  ]
}